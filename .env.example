# =============================================================================
# Environment Variables Template
# Copy this file to .env and fill in your values.
# NEVER commit the real .env file (it is in .gitignore).
# =============================================================================

# -----------------------------------------------------------------------------
# LLM Provider
# Set to "openrouter" for OpenRouter, "openai" for direct OpenAI
# -----------------------------------------------------------------------------
LLM_PROVIDER=openrouter

# -----------------------------------------------------------------------------
# OpenAI-compatible API settings
# When using OpenRouter, OPENAI_BASE_URL must point to OpenRouter's endpoint.
# The openai Python SDK reads OPENAI_API_KEY and OPENAI_BASE_URL automatically.
# -----------------------------------------------------------------------------
OPENAI_API_KEY=your-openrouter-key-here
OPENAI_BASE_URL=https://openrouter.ai/api/v1

# -----------------------------------------------------------------------------
# Model selection
# Paper used: gpt-3.5-turbo-0125  (OpenAI, not free)
#
# PRIMARY (paid, ~$0.92 for full 5-dataset baseline):
#   google/gemini-2.0-flash-001  <- 6/6 probe + merge test: 167→28 labels in 1.9s
#   NOTE: OpenRouter marks this model as retiring 2026-03-31. If unavailable,
#   run probe_models.py on google/gemini-2.0-flash-lite or gemini-2.5-flash.
#
# FREE tier — probe score 6/6 but FAILS merge step (capability ceiling):
#   arcee-ai/trinity-large-preview:free  <- stalls at 144 labels, cannot consolidate
#
# FREE tier — Venice upstream 429 (retry during off-peak hours):
#   meta-llama/llama-3.3-70b-instruct:free
#   mistralai/mistral-small-3.1-24b-instruct:free
#   google/gemma-3-27b-it:free
#   nousresearch/hermes-3-llama-3.1-405b:free
#
# Run tools/probe_models.py before committing to any model change.
# -----------------------------------------------------------------------------
LLM_MODEL=google/gemini-2.0-flash-001

# -----------------------------------------------------------------------------
# Generation settings — keep deterministic for reproducibility
# LLM_MAX_TOKENS: 4096 is required for the merge step (190 labels ≈ 2300 tokens).
#   The pipeline overrides this per-call for the merge step; 4096 is the safe default.
# LLM_REQUEST_DELAY: seconds between API calls. 2s works for gemini-2.0-flash.
#   Increase to 4 for free-tier models subject to stricter rate limits.
# -----------------------------------------------------------------------------
LLM_TEMPERATURE=0
LLM_MAX_TOKENS=4096
LLM_REQUEST_DELAY=2

# -----------------------------------------------------------------------------
# OpenRouter HTTP headers (required by OpenRouter to identify your app)
# https://openrouter.ai/docs/requests
# -----------------------------------------------------------------------------
OR_SITE_URL=https://github.com/AkramChaabnia/text-clustering-llm
OR_APP_NAME=PPD-Text-Clustering-Reproduction
